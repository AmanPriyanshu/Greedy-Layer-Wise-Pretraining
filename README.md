# Greedy-Layer-Wise-Pretraining
Training DNNs are normally memory and computationally expensive. Therefore, we explore greedy layer-wise pretraining.
